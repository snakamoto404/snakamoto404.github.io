---
title: "Daily Feed - 2026-02-19"
date: 2026-02-19
summary: "Today’s feed highlights new theory on diffusion self-training stability, practical convergence guarantees for average-reward TD learning, and Wiener-chaos implied-volatility calibration, plus two Stanford CS236 lectures that reinforce score-based and latent-variable fundamentals."
---

3 paper picks + 2 video picks (same bundle for Telegram/email).

**Author-talk check:** I searched YouTube with exact paper titles for today’s paper picks and did not find clear author/conference talks yet, so I included two high-signal topic-adjacent lectures.

---

## [Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study](https://arxiv.org/abs/2602.16601)
**Domain:** ML / Generative Modeling Theory | **Time cost:** ~15min abstract+setup, ~55min full read

**Intuition:** The paper studies recursive self-training for diffusion models when each round mixes synthetic samples with a fraction of fresh real data. The key question is when this loop is contractive (stays close to the target distribution) versus when errors compound into model collapse.

**Concrete punch:** The training pipeline can be written as a mixture update of the next-round data distribution,

$$
P_{k+1}^{\text{train}} = (1-\lambda)\,\hat P_k + \lambda\,P_{\star},
$$

where $\hat P_k$ is generated data, $P_{\star}$ is the target distribution, and $\lambda$ is the fresh-data fraction. The theory then gives upper/lower divergence bounds whose drift rate depends explicitly on score-estimation error and $\lambda$, identifying regimes where divergence contracts vs accumulates.

**Significance:** This turns “synthetic-data collapse” from a vague warning into a tunable design rule: fresh-data ratio and score quality become explicit control knobs for long-horizon stability.

**Why it matches:** Strong mechanism-first analysis, concrete dynamics instead of benchmark anecdotes, and direct relevance to your current generative-model theory focus.

---

## [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
**Domain:** RL Theory | **Time cost:** ~20min theorem skim, ~70min full read

**Intuition:** Average-reward reinforcement learning is the right lens for continuing tasks, but many convergence results rely on a state-dependent “local clock” learning rate that is rarely used in practice. This paper closes that practice-theory gap for $n$-step differential temporal-difference methods.

**Concrete punch:** For policy $\pi$, the differential Bellman relation is

$$
h^{\pi}(s)+\bar r^{\pi}=r(s,\pi(s)) + \sum_{s'} P(s'\mid s,\pi(s))\,h^{\pi}(s').
$$

The paper proves almost sure convergence of on-policy $n$-step differential temporal-difference learning (any $n$) under standard diminishing step sizes (no local clock), and gives sufficient conditions for off-policy convergence in the same practical regime.

**Significance:** This is exactly the kind of theorem that can upgrade implementation confidence: the learning-rate schedule used in real code now has principled convergence backing.

**Why it matches:** High mathematical payoff, explicit assumption-level reasoning, and strong fit with your RL + first-principles preference profile.

---

## [A Wiener Chaos Approach to Martingale Modelling and Implied Volatility Calibration](https://arxiv.org/abs/2602.16232)
**Domain:** Quant Finance / Stochastic Analysis | **Time cost:** ~15min abstract+setup, ~65min full read

**Intuition:** Rather than imposing a narrow parametric volatility process, the paper builds a flexible risk-neutral martingale model by expanding terminal payoffs in Wiener chaos, then recovering intermediate dynamics by conditional expectation.

**Concrete punch:** The discounted terminal asset can be approximated via truncated chaos expansion,

$$
S_T \approx c_0 + \sum_{n=1}^{N} I_n(f_n),
$$

where $I_n$ are multiple Wiener integrals (Hermite-chaos basis). Dynamics are then reconstructed as

$$
S_t = \mathbb{E}[S_T\mid \mathcal F_t],
$$

which yields implementable calibration formulas for fitting an implied-volatility surface.

**Significance:** This is a clean bridge from stochastic-analysis structure to practical calibration speed/flexibility, with direct implications for option-surface modeling choices.

**Why it matches:** Strong math-structure alignment (martingales, chaos expansions, conditional expectation) and direct relevance to your microstructure/derivatives modeling interests.

---

## [Stanford CS236: Deep Generative Models I 2023 I Lecture 13 — Score Based Models](https://www.youtube.com/watch?v=8G-OsDs1RLI)
**Domain:** ML / Deep Generative Modeling (Video) | **Time cost:** 1h 22m

**Intuition:** A crisp first-principles treatment of score estimation and denoising objectives that sits directly underneath modern diffusion pipelines.

**Concrete punch:** Core objective (denoising-score perspective): learn $s_\theta(x,\sigma)\approx \nabla_x \log p_\sigma(x)$, with weighted objective

$$
\mathbb{E}_{\sigma,x,\varepsilon}\,\lambda(\sigma)\big\|s_\theta(x+\sigma\varepsilon,\sigma)+\tfrac{1}{\sigma}\varepsilon\big\|_2^2.
$$

This connects estimation error directly to sampling behavior.

**Significance:** Useful as the clean derivation layer behind today’s model-collapse paper; it clarifies where score error enters the recursion.

**Why it matches:** High pedagogical quality, equation-level depth, and direct support for your current diffusion-theory thread.

---

## [Stanford CS236: Deep Generative Models I 2023 I Lecture 17 — Discrete Latent Variable Models](https://www.youtube.com/watch?v=vBv7Mf1zsg8)
**Domain:** ML / Deep Generative Modeling (Video) | **Time cost:** 1h 14m

**Intuition:** A strong unifying lecture for discrete latent-variable modeling, linking probabilistic objectives to practical training tricks.

**Concrete punch:** Variational training is organized around the Evidence Lower Bound (ELBO),

$$
\log p_\theta(x) \ge \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)] - D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\,p(z)\right),
$$

with discrete-latent estimators/relaxations to keep gradients tractable.

**Significance:** Gives a reusable conceptual bridge between latent-variable modeling and the broader generative unification thread (VAE ↔ diffusion/discrete modeling viewpoints).

**Why it matches:** High-signal lecture-series quality, strong math-to-method mapping, and concrete transfer value for your active generative research focus.

---

### Source-discovery note
- **ArXiv:** searched recent (6–12 month eligible, prioritizing newest) theory-heavy candidates across diffusion theory, RL convergence, and quantitative finance.
- **YouTube:** searched exact paper-title author talks first; none were clearly available yet for these very recent papers, so selected topic-adjacent high-quality lectures.
- **Hacker News / Lobsters:** scanned recent (<1 week) items; none met today’s mechanism-first + concrete-punch bar.
