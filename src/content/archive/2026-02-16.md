---
title: "Daily Feed - 2026-02-16"
date: 2026-02-16
summary: "Today’s picks span provable RL-vs-SFT learning dynamics for sparse Boolean functions, reliability signals in conditional flows, and entropy-based language structure, plus two strong video lectures on flow matching and Fourier analysis."
---

3 paper picks + 2 video picks (same bundle for Telegram/email).

---

## [Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently](https://arxiv.org/abs/2511.17852)
**Domain:** ML / RL Theory / Boolean Function Learning | **Time cost:** ~20min abstract+setup skim, ~70min full theory read

**Intuition:** This paper asks a sharp mechanistic question: when Chain-of-Thought-style intermediate supervision is available, what do reinforcement learning (RL) versus supervised fine-tuning (SFT) actually optimize differently? It answers in a controlled setting (one-layer transformer learning recursively decomposable sparse Boolean functions), giving a clean separation between the two training dynamics.

**Concrete punch:** The analyzed target class is recursively decomposable sparse Boolean functions over $k$-relevant variables, with canonical primitives including $k$-PARITY, $k$-AND, and $k$-OR. A compact view is

$$
f(x)=g\big(f_L(x_{S_L}), f_R(x_{S_R})\big), \quad g\in\{\mathrm{AND},\mathrm{OR},\mathrm{XOR}\},\quad |S_L|+|S_R|=k.
$$

Main falsifiable claim: under their sufficient conditions, RL learns the full Chain-of-Thought chain jointly, while SFT acquires it stage-by-stage.

**Significance:** If you care about controllable reasoning traces, this gives a principled reason to choose RL-like versus SFT-like fine-tuning depending on whether you want global coupling or incremental compositional acquisition.

**Why it matches:** Strong alignment with your analysis-of-Boolean-functions interest and preference for proofs about learning dynamics rather than benchmark-only claims.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [Diverging Flows: Detecting Extrapolations in Conditional Generation](https://arxiv.org/abs/2602.13061)
**Domain:** ML / Generative Modeling / Reliability | **Time cost:** ~15min abstract+figures, ~60min method/results read

**Intuition:** Conditional flow models can output plausible-looking predictions even when inputs are outside the training manifold. This paper turns that failure mode into an explicit signal by making off-manifold transport intentionally inefficient, so the same model both predicts and flags extrapolation risk.

**Concrete punch:** Flow evolution obeys the continuity equation

$$
\partial_t p_t(x)+\nabla\!\cdot\!\big(p_t(x)v_t(x)\big)=0.
$$

The method introduces structure so off-manifold conditions incur larger effective transport effort (an energy-like path cost), producing a native extrapolation indicator while preserving in-distribution predictive fidelity.

**Significance:** This is a direct mechanism-level fix for silent failure in safety-critical forecasting settings (robotics/weather), not just a post-hoc uncertainty wrapper.

**Why it matches:** It is exactly your preferred style: identify hidden assumptions (in-distribution smoothness), make them explicit, and engineer an objective-level correction with measurable consequences.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
**Domain:** Information Theory / NLP / Statistical Mechanics Lens | **Time cost:** ~20min abstract+model skim, ~65min full read

**Intuition:** Instead of treating the "~1 bit/character" entropy of English as an empirical curiosity, this paper proposes a self-similar semantic chunking model that analytically explains it and predicts how entropy should scale with corpus semantic complexity.

**Concrete punch:** Using a 32-symbol character baseline,

$$
H_{\text{rand}}=\log_2 32=5\ \text{bits/char},\qquad H_{\text{eng}}\approx 1\ \text{bit/char},
$$

so implied redundancy is

$$
1-\frac{H_{\text{eng}}}{H_{\text{rand}}}\approx 1-\frac{1}{5}=0.8.
$$

Their hierarchical chunking model reproduces this scale and predicts that entropy rate should increase with semantic complexity (captured by a single free parameter).

**Significance:** Gives a first-principles target for evaluating whether language models are capturing multiscale semantic structure versus just local token statistics.

**Why it matches:** Strong information-theoretic framing plus a statistical-mechanics flavor (multiscale decomposition), with a concrete quantitative claim you can test on corpora.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [TUM AI Lecture Series — FLUX: Flow Matching for Content Creation at Scale (Robin Rombach)](https://www.youtube.com/watch?v=nrKKLJXBSw0)
**Domain:** ML / Generative Modeling / Systems-Scale Training | **Time cost:** 1h 06m

**Intuition:** A high-signal systems-and-theory talk on how flow-matching ideas move from elegant math to large-scale image generation practice (including training/inference trade-offs and preference-tuning implications).

**Concrete punch:** A standard flow-matching training target is

$$
x_t=(1-t)x_0+t x_1,\qquad
\mathcal{L}_{\mathrm{FM}}=\mathbb{E}_{t,x_0,x_1}\left\|v_\theta(x_t,t)-(x_1-x_0)\right\|_2^2,
$$

which decouples path design from sampler design and helps explain why modern flow models can run with far fewer denoising steps than classic diffusion schedules.

**Significance:** Useful for translating your theoretical generative-model map into concrete engineering decisions (sampler budget, alignment knobs, scaling behavior).

**Why it matches:** First-principles objective, clear bridge from math to production, and directly in your active generative-model unification thread.

---

## [Fourier Analysis of Boolean Functions (Harvard CS 121 guest lecture, Ryan O'Donnell)](https://www.youtube.com/watch?v=xvq7atbDaBI)
**Domain:** Theory / Boolean Functions / Learning Foundations | **Time cost:** 1h 12m

**Intuition:** A compact, equation-forward lecture that turns Boolean functions into spectral objects. Once you move into Fourier space, influence, noise sensitivity, learning hardness, and voting/aggregation behavior become one toolkit instead of disconnected facts.

**Concrete punch:** Core identities (Walsh-Fourier expansion):

$$
f(x)=\sum_{S\subseteq[n]}\hat f(S)\chi_S(x),\qquad
\hat f(S)=\mathbb{E}_x\big[f(x)\chi_S(x)\big],
$$

with Parseval

$$
\mathbb{E}[f(x)^2]=\sum_{S\subseteq[n]}\hat f(S)^2.
$$

This is the exact bridge from combinatorial objects to spectral/energy arguments.

**Significance:** High leverage background for your Boolean-analysis thread and for interpreting modern sparse-feature learning claims in transformer theory papers.

**Why it matches:** Deep mathematical payoff, reusable result structure, and explicit crossovers to learning theory and robustness.

---

### Source-discovery note
- **ArXiv:** primary source for paper picks (frontier submissions).
- **YouTube:** selected for pedagogy/technical density and direct relevance to today’s paper themes.
- **Hacker News/Lobsters:** scanned; no <1-week link cleared the mechanism-first + concrete-punch bar for today’s bundle.
