---
title: "Daily Feed - 2026-02-15"
date: 2026-02-15
summary: "Today’s picks connect geometric control in flow matching, kinetic-energy diagnostics, and sampling-driven alignment dynamics, with two CS236 lectures reinforcing the objective-level foundations behind modern generative modeling."
---

3 paper picks + 2 video picks (same bundle for Telegram/email).

---

## [Improving Classifier-Free Guidance of Flow Matching via Manifold Projection](https://arxiv.org/abs/2601.21892)
**Domain:** ML / Generative Modeling / Optimization | **Time cost:** ~25min abstract+method skim, ~70min deep read

**Intuition:** Classifier-free guidance (CFG) in diffusion/flow models is usually treated as a useful heuristic knob. This paper reframes it as an optimization problem: guided sampling approximates gradient flow toward a target manifold, and instability at high guidance scales is a geometry/projection issue rather than a mysterious tuning artifact.

**Concrete punch:** Standard CFG uses

$$
v_{\text{cfg}} = v_{\text{uncond}} + s\big(v_{\text{cond}}-v_{\text{uncond}}\big),
$$

where $s$ is guidance scale. The paper interprets this as an approximation to the gradient of smoothed distance objectives and adds an explicit manifold-projection correction during sampling (implemented with incremental gradient steps + Anderson acceleration), improving robustness to $s$.

**Significance:** Turns a brittle practical trick into a principled sampling procedure. If this view holds broadly, it gives a cleaner path to stable high-guidance generation without retraining.

**Why it matches:** Mechanism-first and mathematically explicit (optimization geometry over ad-hoc tuning), directly aligned with your preference for derivable objectives and reusable structure.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
**Domain:** ML / Generative Modeling / Physics Lens | **Time cost:** ~20min abstract+figures, ~60min method/results read

**Intuition:** Each flow-matching sample follows an ODE trajectory from noise to data; this paper assigns a per-trajectory “effort” and uses it diagnostically. The key twist is non-monotonicity: too little kinetic effort underfits semantics, too much can push you into memorization.

**Concrete punch:** Define Kinetic Path Energy (KPE) per trajectory as

$$
\mathrm{KPE}(x_{0:1}) = \int_0^1 \tfrac{1}{2}\,\|v_t(x_t)\|_2^2\,dt.
$$

Empirically/theoretically, KPE tracks fidelity up to a point; beyond that, very high-energy paths drift toward near-copies of training samples. They propose a training-free two-phase sampler (Kinetic Trajectory Shaping) to target this “Goldilocks” regime.

**Significance:** Gives a concrete diagnostic/control variable for sample quality vs memorization risk in flow models, not just a post-hoc visual judgment.

**Why it matches:** Strong physics-style variational/energy framing with a concrete operational consequence (how to steer inference), exactly the kind of structural lens you prefer.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180)
**Domain:** ML / RLHF Theory / Information-Theoretic Alignment | **Time cost:** ~25min abstract+theory skim, ~75min full read

**Intuition:** Preference alignment methods depend not only on the loss but on how candidate responses are sampled and what reference policy is used. This paper shows those choices can qualitatively change long-run dynamics when the trained policy feeds back into future data collection.

**Concrete punch:** The analysis covers pairwise preference objectives in the DPO/IPO family, with representative form

$$
\mathcal{L}_{\mathrm{DPO}}=-\log\sigma\!\left(\beta\left[(\log\pi_\theta(y_w\mid x)-\log\pi_\theta(y_l\mid x))-(\log\pi_{\mathrm{ref}}(y_w\mid x)-\log\pi_{\mathrm{ref}}(y_l\mid x))\right]\right).
$$

Main claim: instance-dependent sampling can improve ranking guarantees, while skewed on-policy sampling can induce oscillations or entropy collapse in iterative alignment.

**Significance:** Helpful for designing stable preference-data loops (sampling/reference refresh schedules), not just one-shot objective tuning.

**Why it matches:** Direct continuation of your DPO-style interests, with explicit dynamical-systems framing and testable stability regimes.

**Author-talk search:** No exact-title author/conference YouTube talk found in a quick pass.

---

## [Stanford CS236 (Lecture 4): Maximum Likelihood Learning](https://www.youtube.com/watch?v=bt3dqcbMLa0)
**Domain:** ML / Deep Generative Modeling Theory | **Time cost:** 1h 23m

**Intuition:** This lecture gives the clean likelihood-first backbone that unifies much of generative modeling: latent-variable models, variational inference, and why ELBO optimization is the practical proxy for intractable posteriors.

**Concrete punch:** Core identity:

$$
\log p_\theta(x)=\mathcal{L}_{\mathrm{ELBO}}(x;\theta,\phi)+D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\right),
$$

with

$$
\mathcal{L}_{\mathrm{ELBO}}=\mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]-D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\,p(z)\right).
$$

This makes explicit exactly what is optimized vs approximated.

**Significance:** Great anchor for connecting VAE-style objectives to modern flow/diffusion work without losing first principles.

**Why it matches:** High pedagogy, equation-first exposition, and directly useful for your “unify generative paradigms” thread.

---

## [Stanford CS236 (Lecture 9): GANs](https://www.youtube.com/watch?v=3Zv-gokhLu8)
**Domain:** ML / Deep Generative Modeling | **Time cost:** 1h 18m

**Intuition:** A principled walkthrough of adversarial training beyond folklore: objective design, discriminator-generator game dynamics, and where instability actually comes from.

**Concrete punch:** Canonical GAN game:

$$
\min_G\max_D\;\mathbb{E}_{x\sim p_{\mathrm{data}}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log(1-D(G(z)))].
$$

At discriminator optimum, this connects to minimizing Jensen-Shannon divergence between model and data distributions, clarifying what the game is approximating.

**Significance:** Useful complement to diffusion/flow-heavy reading: keeps adversarial generative modeling in the same objective-level map.

**Why it matches:** Exactly the kind of reusable conceptual bridge (objective geometry + practical consequences) you asked for across VAE/GAN/diffusion lines.

---

### Source-discovery note
- ArXiv supplied the three paper picks today.
- Hacker News / Lobsters were scanned, but no <1-week discussion link cleared the “mechanism-first + concrete punch” bar for this run.
